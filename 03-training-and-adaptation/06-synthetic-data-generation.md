# Synthetic Data Generation

In late 2025, the industry has hit the "Data Wall"â€”the exhaustion of high-quality human text on the web. Synthetic data is now the primary engine for model improvement.

## Table of Contents

- [The Data Wall and the Synthetic Shift](#synthetic-shift)
- [Evol-Instruct Pattern](#evol-instruct)
- [Constitutional AI & Self-Correction](#constitutional-ai)
- [Verifiable Synthetic Data (Math/Code)](#verifiable-data)
- [De-biasing and Diversity](#diversity)
- [Interview Questions](#interview-questions)
- [References](#references)

---

## After the "Data Wall": The Synthetic Shift

Frontier models (Llama 4, GPT-5) are trained on 100T+ tokens. There simply isn't enough human text to sustain this scaling. 
**The 2025 Reality:** More than **50% of the training mixture** for frontier fine-tuning (and 10% of pretraining) is now synthetic.

| Source | Human Data | Synthetic Data |
|--------|------------|----------------|
| **Volume** | Fixed (Finite) | Infinite |
| **Quality** | Variable (Noisy) | Controllable (Purified) |
| **Cost** | High (Human Labelers) | Cheap (Inference/GPU) |
| **Bias** | Mirror of internet | Can be manually balanced |

---

## Evol-Instruct Pattern (2025 Evolution)

Evol-Instruct is a recursive process where an LLM takes a simple instruction and evolves it into a more complex one.

**The Evolution Directions:**
1. **Breadth**: Increase the number of tasks.
2. **Depth**: Add constraints, complicating factors, or multi-step logic.
3. **De-noising**: Clean up phrasing to remove "AI-isms."

```python
# Simple Instruction: "Write a function to add two numbers."
# Evolved Instruction: "Write a thread-safe Python class that performs 
# matrix addition with error handling and unit tests, adhering to PEP8."
```

---

## Constitutional AI & AI Feedback (RLAIF)

Developed by Anthropic and adopted widely in 2025, RLAIF uses a "Constitution" (a set of rules) to guide a model in evaluating and improving its own data.

**The Loop:**
1. **Propose**: Model A generates a response.
2. **Critique**: Model B (the constitutional judge) identifies flaws based on guidelines.
3. **Revise**: Model A produces a better version based on the critique.
4. **Train**: The final (Prompt, Revise) pair is added to the SFT set.

---

## Verifiable Synthetic Data

The biggest risk of synthetic data is **Model Collapse** (the model learning its own mistakes).
**The 2025 Solution**: Focus on domains where the "Truth" is verifiable without an LLM.

- **Math**: Use Formal Verification (Lean/Isabelle) or Python execution to verify answers.
- **Code**: Run generated code against test cases (Unit Tests).
- **RAG**: Use "Gold Context" to generate questions where the answer is explicitly in the text.

---

## De-biasing and Diversity

Synthetic data is used to "fill the gaps" in human data.
- **Languages**: Generating high-quality text in low-resource languages (e.g., Swahili, Marathi) by translating conceptual templates.
- **Logic**: Creating 1,000,000 variations of a specific logical fallacy to "harden" the model against it.

---

## Interview Questions

### Q: What is the risk of "Model Collapse" when training on synthetic data?

**Strong answer:**
Model Collapse occurs when a model is trained on data generated by an earlier version of itself. Because the model's distribution is narrower than the real world (it has preferences/biases for certain words and patterns), the training loop becomes a "positive feedback loop" of errors and blandness. By 2025, we mitigate this by:
1. Mixing in 5-20% "Golden" human-authenticated data.
2. Using "Verifiable" rewards (Math/Code) so mistakes are never learned.
3. Using more powerful "Teacher" models to generate data for "Student" models.

### Q: How do you ensure the *quality* of a synthetic dataset of 10 million rows?

**Strong answer:**
We use a **Multi-Stage Filtering Pipeline**:
1. **Semantic Deduplication**: Using embeddings to remove near-identical clusters.
2. **LLM-as-Judge**: Sampling 1% of the data and having a stronger model (e.g., GPT-5.2) score it for logic and safety.
3. **Perplexity Filtering**: Using a small model to calculate the perplexity of the text. If it's too high (nonsense) or too low (repetitive/simple), it's discarded.
4. **Verifiable Execution**: If the data contains code or math, it must pass a local compiler/interpreter check.

---

## References
- Xu et al. "WizardLM: Empowering Large Language Models to Follow Complex Instructions" (2023)
- Bai et al. "Constitutional AI: Harmlessness from AI Feedback" (2022)
- OpenAI. "Weak-to-Strong Generalization" (2023)

---

*Next: [Quantization Deep Dive](07-quantization-deep-dive.md)*
